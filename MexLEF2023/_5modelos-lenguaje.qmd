# Modelos de lenguaje

## BERT
::: {.r-fit-text}
Los modelos de lenguaje, _Language Models (LM)_ utilizan el contexto de cada palabra para determinar su representación.

**BERT** es un modelo de lenguaje que en su momomento rompió el paradigma. Consiste en una serie de _encoders_ que generan representaciones para cada palabra dependiendo de su contexto. El entrenamiento usa un lenguaje de enmascarado, Masked Language Model (MLM). Cada sentencia enmascará tokens de manera aleatoría (se enmascaran 15% de los tokens). También se entrena para predicción de la siguiente frase.
:::

## Recursos computacionales y necesidad de datos
:::  {.r-fit-text}
- Los modelos de lenguaje requieren una gran cantidad de datos, solo generamos recursos con MLM sobre AR, CL, CO, MX, ES, UY, VE, y US, i.e., los más grandes.
- Todos los modelos tienen series de dos encoders con cuatro cabezas de atención cada una y una salida de 512 dimensiones por embedding
- Corresponde al _small-size_ del BERT original, y es lo que actualmente podemos con los recursos que contamos en un tiempo _pagable_ (usamos una estanción de trabajo con dos NVIDIA TITAN RTX con 24 GB cada una).
- Nombramos a nuestro modelo BILMA por _Bert In Latin America_.
- Usamos un _learning rate_ de $10^{-5}$ con el optimizador Adam (usamos tensorflow 2 y Keras).
- Los modelos para CL, UY, VE, y US se entrenaron con 3 epocas y AR, CO, MX, y ES con solo una, dado los tamaños de los corpus.
:::

# Emoji-15 con BILMA

## Como se compara BILMA con los word-embeddings
:::: {.columns}
::: {.column}
![_Accuracy_ en predicción de Emoji-15 -- _tuneado_](https://github.com/sadit/regional-spanish-models-talk-2022/raw/main/src/figs/fig-bilma-cls.png)
:::

::: {.column}

::::: {style="font-size: 16pt;"} 
| cc |	minrecall	| maxrecall |	localrank |	top5 |
|---|---|---|---|---|
| AR |	0.4777 |	0.49   |	3	| UY,PY,AR,PE,CO  |
| CL |	0.4257 |	0.4494 |	1	| CL,US,MX,AR,ES  |
| CO |	0.4247 |	0.4365 |	2	| US,CO,VE,EC,GT  |
| ES |	0.4754 |	0.4855 |	1	| ES,AR,MX,US,VE  |
| MX |	0.4233 |	0.4335 |	1	| MX,GT,CR,US,CO  |
| US |	0.4041 |	0.4244 |	1	| US,MX,CO,ES,CL  |
| UY |	0.4351 |	0.4572 |	1	| UY,US,CO,CL,VE  |
| VE |	0.3848 |	0.4338 |	4	| MX,CO,ES,VE,US  |
:::::

Con word-embeddings
:::
::::

##
- Se _tuneó_ el modelo BILMA para predecir emoticones añadiendo dos capas lineales a los embeddings de inicio, por lo que se puede ver que se predice independiente de la posición. 
- _Tuneado_ con 90%-10% del training set de la región hasta que el _accuracy_ converge.
- Se evaluó con test regional.
- Observe que es una matriz de modelos pre-entrenados y _tuneos_.
- Los resultados en general son muy similares a los modelos de fastText, pero, los modelos BILMA pueden hacer más cosas...

## Usando BILMA para completar frases (mediante máscaras)
![_Accuracy_ en la tarea MLM para el test](https://github.com/sadit/regional-spanish-models-talk-2022/raw/main/src/figs/fig-bilma-mlm.png)


## {background-image=https://github.com/sadit/regional-spanish-models-talk-2022/raw/main/src/figs/bilma-mlm-table.png background-size=contain}
MLM regional
<!-- ![](https://github.com/sadit/regional-spanish-models-talk-2022/raw/main/src/figs/bilma-mlm-table.png){.absolute left=0 top=0 width=1200 height=2400 }
-->

