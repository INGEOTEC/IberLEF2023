---
lang: en
title: "Text Classification"
# subtitle: "Poornima Institute of Engineering & Technology"
author:
    - name: "Mario Graff"
      url: https://mgraffg.github.io
      affiliation: INFOTEC
    - name: "Daniela Moctezuma"
      url: https://dmocteo.github.io/
      affiliation: "CentroGEO"
    - name: "Eric S. Téllez"
      url: https://sadit.github.io
      affiliation: INFOTEC 
    - name: "Sabino Miranda-Jiménez"

format:
  revealjs:
    theme: simple
    chalkboard: true
    footer: <https://ingeotec.github.io>
---


# Who / Where 

## INGEOTEC

```{python}
#| include: false
from EvoMSA.utils import Download
from microtc.utils import tweet_iterator
from microtc import TextModel
from microtc.utils import Counter
from microtc.params import OPTION_GROUP, OPTION_DELETE, OPTION_NONE
from CompStats import StatisticSamples

from sklearn import decomposition
from sklearn.datasets import load_iris
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import KFold
from sklearn.metrics import recall_score

from matplotlib import pylab as plt
from wordcloud import WordCloud
import plotly.express as px
import plotly.graph_objects as go
from IPython.display import Markdown

import unicodedata
from random import shuffle
from os.path import isdir, isfile
import re

import pandas as pd
import numpy as np
```

```{python} 
#| include: false

URL = 'https://github.com/INGEOTEC/talks/releases/download/Talks/emoji_text.ttf.zip'
if not isfile('emoji_text.ttf.zip'):
  Download(URL, 'emoji_text.ttf.zip')
if not isfile('emoji_text.ttf'):
  !unzip -Pingeotec emoji_text.ttf.zip
```

![](../IberLEF2023/ingeotec-cuatro.png){width=200}

::: {style="font-size: 70%; text-align: center;"}  
GitHub: <https://github.com/INGEOTEC>  
WebPage: <https://ingeotec.github.io/>
:::

## Aguascalientes, México

![](aguascalientes_edo.jpg){width=200}

# Opinion Mining 

## Opinion Mining 

::: {.callout-note icon="false"} 
### Definition 

Study people’s opinions, appraisals, attitudes, and emotions toward entities, individuals, events, and their attributes.
:::

* Distilling opinions from texts
* A brand is interested in the costumer's opinions
* Huge amount of information 

## Opinion Mining (2) 

::: {.callout-note icon="false"}
### Formal Definition 

* $e_i$ -       Entity
* $a_{ij}$ -    Aspect of $e_i$
* $o_{ijkl}$ -  Opinion orientation
* $h_k$ -       Opinion source - Opinion holder
* $t_l$ -       Time of the opinion
:::

::: {.callout-tip icon="false"}
### Entity 

Product, service, person, event, organization, or topic
:::

::: {.callout-tip icon="false"}
### Aspect 

Entity's component or attribute
:::

## Opinion Mining Tasks 

::: {.callout-tip icon="false"}
### Tasks 
* Entity extraction
* Aspect extraction - considered the entities
* Identify opinion source and time
* Identify opinion orientation
:::

# Introduction 

## Text Classification

::: {.callout-note icon=false} 
### Definition 

The aim is the **classification** of **documents** into a fixed number of predefined categories. 
::: 

::: {.callout-tip icon=false} 
### Polarity 
El día de mañana no podré ir con ustedes a la librería
:::

::: {.fragment style="font-size: 100%; text-align: center;"} 
**Negative**
:::

## Text Classification Tasks 

::: {.callout-tip icon=false} 
### Polarity 
Positive, Negative, Neutral
:::

::: {.callout-tip icon=false} 
### Emotion (Multiclass) 

* Anger, Joy, ...
* Intensity of an emotion
:::

::: {.callout-tip icon=false} 
### Event (Binary) 

* Violent
* Crime
:::

## Profiling

::: {.callout-tip icon=false} 
### Gender 

Man, Woman, Nonbinary, ...   
:::

::: {.callout-tip icon=false} 
### Age 

Child, Teen, Adult
:::

::: {.callout-tip icon=false} 
### Language Variety 

* Spanish: Spain, Cuba, Argentine, México, ...
* English: United States, England, ...
:::

# Approach 

## Machine Learning 

::: {.callout-note icon=false} 
### Definition 

Machine learning (ML) is a subfield of artificial intelligence that focuses on the development and implementation of algorithms capable of learning from data without being explicitly programmed.
:::

::: {.callout-note icon=false} 
### Types of ML algorithms 

* Unsupervised Learning
* Supervised Learning
* Reinforcement Learning
:::

## Supervised Learning (Multiclass) 

```{python}
#| echo: false

D, y = load_iris(return_X_y=True)
pca = decomposition.PCA(n_components=2).fit(D)
Dn = pca.transform(D)
df = pd.DataFrame(Dn, columns=['x', 'y'])
df['Class'] = y.astype(str)
fig = px.scatter(df, color='Class', x='x', y='y', height=400)
fig.update_layout(yaxis={'visible': True, 'showticklabels': False}, 
                  xaxis={'visible': True, 'showticklabels': False},
                  xaxis_title=None, yaxis_title=None)
fig.show()
```

## Supervised Learning (Binary) 

```{python}
#| echo: false

df['Class'] = ['P' if i == 0 else 'N' for i in y]
fig = px.scatter(df, color='Class', x='x', y='y', height=400)
fig.update_layout(yaxis={'visible': True, 'showticklabels': False}, 
                  xaxis={'visible': True, 'showticklabels': False},
                  xaxis_title=None, yaxis_title=None)
fig.show()
```

## Supervised Learning (Classification) 

```{python}
#| echo: false

df['Class'] = ['P' if i == 0 else 'N' for i in y]
m = LinearSVC(dual='auto').fit(Dn, [1 if i == 0 else 0 for i in y])
w_1, w_2 = m.coef_[0]
w_0 = m.intercept_[0]
g_0 = [dict(x1=x, x2=y, tipo='g(x)=0')
       for x, y in zip(Dn[:, 0], (-w_0 - w_1 * Dn[:, 0]) / w_2)]
line = go.Scatter(x=Dn[:, 0], y=(-w_0 - w_1 * Dn[:, 0]) / w_2,
                  line=dict(color='darkgrey'),
                  showlegend=False)
fig = px.scatter(df, color='Class', x='x', y='y', height=400)

fig.update_layout(yaxis={'visible': True, 'showticklabels': False}, 
                  xaxis={'visible': True, 'showticklabels': False},
                  xaxis_title=None, yaxis_title=None)
fig.add_trace(line)
fig.show()
```

::: {.fragment .callout-tip icon="false"} 
### Decision function 
```{python}
#| echo: false

Markdown(f'$g(\mathbf x) = {w_1:0.2f} x_1 + {w_2:0.2f} x_2 + {w_0:0.2f}$')
```
:::

## Supervised Learning (Geometry) 

```{python}
#| echo: false

df['Class'] = ['P' if i == 0 else 'N' for i in y]
m = LinearSVC(dual='auto').fit(Dn, [1 if i == 0 else 0 for i in y])
w_1, w_2 = m.coef_[0]
w_0 = m.intercept_[0]
g_0 = [dict(x1=x, x2=y, tipo='g(x)=0')
       for x, y in zip(Dn[:, 0], (-w_0 - w_1 * Dn[:, 0]) / w_2)]
line = go.Scatter(x=Dn[:, 0], y=(-w_0 - w_1 * Dn[:, 0]) / w_2,
                  line=dict(color='darkgrey'),
                  showlegend=False)
fig = px.scatter(df, color='Class', x='x', y='y', height=400)
# fig.update_layout(yaxis={'visible': True, 'showticklabels': False}, 
#                   xaxis={'visible': True, 'showticklabels': False},
                  # xaxis_title=None, yaxis_title=None)
fig.add_trace(line)
point = go.Scatter(x=[w_1], y=[w_2],
                  line=dict(color='darkgrey'),
                  showlegend=False)
fig.add_trace(point)
fig.show()
```

::: {.callout-tip icon="false"} 
### Decision function 

```{python} 
#| echo: false

Markdown(f'$g(\mathbf x) = {w_1:0.2f} x_1 + {w_2:0.2f} x_2 + {w_0:0.2f}$')
``` 
:::

## Supervised Learning (Geometry 2) 

```{python}
#| echo: false

df['Class'] = ['P' if i == 0 else 'N' for i in y]
m = LinearSVC(dual='auto').fit(Dn, [1 if i == 0 else 0 for i in y])
w_1, w_2 = m.coef_[0]
w_0 = m.intercept_[0]
g_0 = [dict(x1=x, x2=y, tipo='g(x)=0')
       for x, y in zip(Dn[:, 0], (-w_0 - w_1 * Dn[:, 0]) / w_2)]
line = go.Scatter(x=Dn[:, 0], y=(-w_0 - w_1 * Dn[:, 0]) / w_2,
                  line=dict(color='darkgrey'),
                  showlegend=False)
w_00 = 0.88
g_0 = [dict(x1=x, x2=y, tipo='g(x)=0')
       for x, y in zip(Dn[:, 0], (-w_00 - w_1 * Dn[:, 0]) / w_2)]
line2 = go.Scatter(x=Dn[:, 0], y=(-w_00 - w_1 * Dn[:, 0]) / w_2,
                   line=dict(color='black'),
                   showlegend=False)
fig = px.scatter(df, color='Class', x='x', y='y', height=370)
# fig.update_layout(yaxis={'visible': True, 'showticklabels': False}, 
#                   xaxis={'visible': True, 'showticklabels': False},
                  # xaxis_title=None, yaxis_title=None)
fig.add_trace(line)
fig.add_trace(line2)
point = go.Scatter(x=[w_1], y=[w_2],
                  line=dict(color='darkgrey'),
                  showlegend=False)
fig.add_trace(point)
fig.show()
```

::: {.callout-tip icon="false"} 
### $w_0$ 

```{python}
#| echo: false

_ = [f'* $w_0 = {w_0:0.2f}$',
     f'* $w_0 = {w_00:0.2f}$']
Markdown('\n'.join(_))
```
:::

## Supervised Learning (Geometry 3) 

```{python}
#| echo: false
#| fig-align: center

df['Class'] = ['P' if i == 0 else 'N' for i in y]
m = LinearSVC(dual='auto').fit(Dn, [1 if i == 0 else 0 for i in y])
w_1, w_2 = m.coef_[0]
w_0 = m.intercept_[0]
g_0 = [dict(x1=x, x2=y, tipo='g(x)=0')
       for x, y in zip(Dn[:, 0], (-w_0 - w_1 * Dn[:, 0]) / w_2)]
line = go.Scatter(x=Dn[:, 0], y=(-w_0 - w_1 * Dn[:, 0]) / w_2,
                  line=dict(color='darkgrey'),
                  showlegend=False)
eq1 = f'* $g_{{svm}}(\mathbf x) = {w_1:0.2f} x_1 + {w_2:0.2f} x_2 + {w_0:0.2f}$'
point1 = go.Scatter(x=[w_1], y=[w_2],
                  line=dict(color='darkgrey'),
                  showlegend=False)

m = LogisticRegression().fit(Dn, [1 if i == 0 else 0 for i in y])
w_1, w_2 = m.coef_[0]
w_0 = m.intercept_[0]
g_0 = [dict(x1=x, x2=y, tipo='g(x)=0')
       for x, y in zip(Dn[:, 0], (-w_0 - w_1 * Dn[:, 0]) / w_2)]
line2 = go.Scatter(x=Dn[:, 0], y=(-w_0 - w_1 * Dn[:, 0]) / w_2,
                   line=dict(color='black'),
                   showlegend=False)
eq2 = f'* $g_{{lr}}(\mathbf x) = {w_1:0.2f} x_1 + {w_2:0.2f} x_2 + {w_0:0.2f}$'
point2 = go.Scatter(x=[w_1], y=[w_2],
                  line=dict(color='black'),
                  showlegend=False)

fig = px.scatter(df, color='Class', x='x', y='y', height=370)
# fig.update_layout(yaxis={'visible': True, 'showticklabels': False}, 
#                   xaxis={'visible': True, 'showticklabels': False},
                  # xaxis_title=None, yaxis_title=None)
fig.add_trace(line)
fig.add_trace(line2)

fig.add_trace(point1)
fig.add_trace(point2)
fig.show()  
``` 

::: {.callout-tip icon="false"} 
### Decision function 

```{python}
#| echo: false

_ = [eq1, eq2]
Markdown('\n'.join(_))
``` 
:::

# Starting point 

## Training set

```{python}
#| include: false

if not isfile('delitos.zip'):
  Download('https://github.com/INGEOTEC/Delitos/releases/download/Datos/delitos.zip',
           'delitos.zip')
if not isdir('delitos'):
  !unzip -Pingeotec delitos.zip
''
```

::: {style="font-size: 70%; text-align: center;"}
```{python}
#| echo: false

_ = list(tweet_iterator('delitos/delitos_ingeotec_Es_train.json'))
shuffle(_)
neg = [x for x in _ if x['klass']==0]
pos = [x for x in _ if x['klass']==1]
D = []
for a, b in zip(neg, pos):
  D.append(a)
  D.append(b)
df = pd.DataFrame(D)
df[['text', 'klass']].head(n=8)
```
:::


## Quiz 

::: {.callout-warning icon="false"} 
### Question 

Which of the following tasks does the previous training set belong to?

1. Polarity 
2. Emotion identification
3. Aggressive detection
4. Profiling
:::

## Training set (2) 

::: {.callout-note icon=false} 
### Problem 
The independent variables are **texts**
:::

::: {.fragment .callout-tip icon=false} 
### Solution 

* Represent the **texts** in an suitable format for the classifier
* Token as a vector
* Sparse vector
* Dense vector
* Utterance as a vector
:::

# Text Representation 

## Token as Vector 

::: {.callout-note icon="false"} 
### Token as vector 

* The idea is that each token $t$ is associate to a vector $\mathbf v_t \in \mathbb R^d$
* Let $\mathcal V$ represent the set composed by the different tokens
* $d$ corresponds to the dimension of the vector
:::

::: {.fragment .callout-tip icon="false"} 
### $d << \lvert \mathcal V \rvert$ (Dense Vector) 

* GloVe
* Word2vec
* fastText
:::

## Token as Vector (2) 

::: {.callout-note icon="false"} 
### $d = \lvert \mathcal V \rvert$ (Sparse Vector)

* $\forall_{i \neq j} \mathbf v_i \cdot \mathbf v_j = 0$
* $\mathbf v_i \in \mathbb R^d$
* $\mathbf v_j \in \mathbb R^d$
:::

::: {.fragment .callout-tip icon="false"} 
### Algorithm 

* Sort the vocabulary $\mathcal V$
* Associate $i$-th token to
* $(\ldots, 0, \overbrace{\beta_i}^i, 0, \ldots)^\intercal$
* where $\beta_i > 0$
:::


## Utterance as Vector 

::: {.callout-note icon="false"} 
### Procedure 
$$\mathbf x = \sum_{t \in \mathcal U} \mathbf{v}_t$$ 

* where $\mathcal{U}$ corresponds to all the tokens of the utterance
* The vector $\mathbf{v}_t$ is associated to token $t$ 
:::

::: {.callout-tip icon="false" .fragment} 
### Unit Vector 

$$\mathbf x = \frac{\sum_{t \in \mathcal U} \mathbf v_t}{\lVert \sum_{t \in \mathcal U} \mathbf v_t \rVert} $$
:::

# Sparse Representation 

## Tokens 

```{mermaid} 
%%| echo: false
%%| fig-align: center

flowchart LR
    Entrada([Text]) -->  Norm[Text Normalizer]
    Norm --> Seg[Tokenizer]
    Seg --> Terminos(...)
``` 

:::: {.columns} 

::: {.fragment .callout-note icon="false" .column width=40%} 
### Text Normalization 

* User
* URL
* Entity
* Case sensitive
* Punctuation
* Diacritic
:::

::: {.fragment .callout-tip icon="false" .column} 
### Diacritic (remove) 

```{python} 
#| echo: true

text = 'México'
output = ""
for x in unicodedata.normalize('NFD', text):
    o = ord(x)
    if 0x300 <= o and o <= 0x036F:
        continue
    output += x
output
```
:::
::::

## Text Normalization 

::: {.callout-tip icon="false"} 
### Case sensitive 

```{python} 
#| echo: true

text = "México"
output = text.lower()
output
``` 
::: 

::: {.callout-tip icon="false"} 
### URL (replace) 

```{python} 
#| echo: true

text = "go http://google.com, and find out"
output = re.sub(r"https?://\S+", "_url", text)
output
``` 

:::


## Tokenizer 

::: {.callout-note icon="false"} 
### Common Types 

* Words
* n-grams (Words)
* q-grams (Characters)
* skip-grams
:::

::: {.fragment .callout-tip icon="false"} 
### Words 

```{python}
#| echo: true

text = 'I like playing football on Saturday'
words = text.split()
words
```
:::

## Tokenizer (2) 

:::: {.columns} 
::: {.callout-tip icon="false" .column} 
### n-grams 

```{python}
#| echo: true

text = 'I like playing football on Saturday'
words = text.split()
n = 3
n_grams = []
for a in zip(*[words[i:] for i in range(n)]):
    n_grams.append("~".join(a))
n_grams
```
:::

::: {.fragment .callout-tip icon="false" .column} 
### q-grams 

```{python}
#| echo: true

text = 'I like playing'
q = 4
q_grams = []
for a in zip(*[text[i:] for i in range(q)]):
    q_grams.append("".join(a))
q_grams
```
:::
::::

## $\mu$-TC 

::: {.callout-tip icon="false"} 
### TextModel 

```{python}
#| echo: true

tm = TextModel(token_list=[-1],
               num_option=OPTION_NONE,
               usr_option=OPTION_DELETE,
               url_option=OPTION_DELETE,
               emo_option=OPTION_NONE,
               lc=True,
               del_dup=False,
               del_punc=True,
               del_diac=True)
```
:::

::: {.fragment} 
```{python}
#| echo: true

text = 'I like playing football with @mgraffg'
tm.tokenize(text)
```
:::

## $\mu$-TC (2) 

:::: {.columns} 

::: {.callout-tip icon="false" .column} 
### TextModel 

```{python}
#| echo: true

tm = TextModel(token_list=[-2, -1, 6],
               num_option=OPTION_NONE,
               usr_option=OPTION_DELETE,
               url_option=OPTION_DELETE,
               emo_option=OPTION_NONE, 
               lc=True, del_dup=False,
               del_punc=True, del_diac=True)
```
:::

::: {.fragment .column} 
```{python} 
#| echo: true

text = 'I like playing...'
tm.tokenize(text)
```
:::
::::

## Training set 

```{python}
#| echo: true

URL = 'https://github.com/INGEOTEC/Delitos/releases/download/Datos/delitos.zip'
if not isfile('delitos.zip'):
  Download(URL,
           'delitos.zip')
if not isdir('delitos'):
  !unzip -Pingeotec delitos.zip
```

## Utterance as Vector 

:::: {.columns} 
::: {.callout-tip icon="false" .column} 
### TextModel 

```{python}
#| echo: true

tm = TextModel(token_list=[-1],
               num_option=OPTION_NONE,
               usr_option=OPTION_DELETE,
               url_option=OPTION_DELETE,
               emo_option=OPTION_NONE, 
               lc=True, del_dup=False,
               del_punc=True, del_diac=True)
```               
:::

::: {.fragment .callout-tip icon="false" .column} 
### Tokenizer 

```{python}
#| echo: true

fname = 'delitos/delitos_ingeotec_Es_train.json'
training_set = list(tweet_iterator(fname))
tm.tokenize(training_set[0])[:3]
```
:::
::::

::: {.fragment .callout-tip icon="false"} 
### Vocabulary 

```{python} 
#| echo: true

voc = Counter()
for text in training_set:
  tokens = set(tm.tokenize(text))
  voc.update(tokens)
voc.most_common(n=3)
```
:::

## Utterance as Vector (2) 

::: {.callout-tip icon="false"} 
### Inverse Document Frequency (IDF) 

```{python}
#| echo: true

token2id = {}
token2beta = {}
N = np.log2(voc.update_calls)
for id, (k, n) in enumerate(voc.items()):
  token2id[k] = id
  token2beta[k] = N - np.log2(n)
```               
:::

::: {.fragment .callout-tip icon="false"} 
### Term Frequency - IDF 
```{python} 
#| echo: true

text = training_set[3]['text']
tokens = tm.tokenize(text)
vector = []
for token, tf in zip(*np.unique(tokens, return_counts=True)):
  if token not in token2id:
    continue
  vector.append((token2id[token], tf * token2beta[token]))
vector[:4]
```
:::

## Utterance as Vector (3) 

::: {.callout-tip icon="false"} 
### $\mu$-TC

```{python}
#| echo: true

tm.fit(training_set)
```               
:::

::: {.callout-tip icon="false"} 
### Utterance as Vector 
```{python} 
#| echo: true

text = training_set[3]['text']
tm[text][:4]
```
:::

## Quiz 

::: {.callout-warning icon="false"} 
### Question 

Which of the following representations do you consider to produce a larger vocabulary?
:::

:::: {.columns} 
::: {.callout-warning icon="false" .column} 
### A 
```{python}
#| echo: true
tmA = TextModel(token_list=[-1, 3],
                num_option=OPTION_NONE,
                usr_option=OPTION_DELETE,
                url_option=OPTION_DELETE,
                emo_option=OPTION_NONE, 
                lc=True, del_dup=False,
                del_punc=True,
                del_diac=True
               ).fit(training_set)
```
:::

::: {.callout-warning icon="false" .column} 
### B 
```{python}

#| echo: true
tmB = TextModel(token_list=[-1, 6],
                num_option=OPTION_NONE,
                usr_option=OPTION_DELETE,
                url_option=OPTION_DELETE,
                emo_option=OPTION_NONE, 
                lc=True, del_dup=False,
                del_punc=True,
                del_diac=True
               ).fit(training_set)
```
:::
::::

# Text Classification 

## Procedure 

::: {.callout-tip icon="false"} 
### Text as Vectors 

```{python}
#| echo: true

tm = TextModel(token_list=[-1], num_option=OPTION_NONE,
                usr_option=OPTION_DELETE, url_option=OPTION_DELETE,
                emo_option=OPTION_NONE, lc=True, del_dup=False,
                del_punc=True, del_diac=True
               ).fit(training_set)
X = tm.transform(training_set)
```
:::

::: {.fragment .callout-tip icon="false"} 
### Training a Classifier 

```{python}
#| echo: true

labels = [x['klass'] for x in training_set]
m = LinearSVC(dual='auto').fit(X, labels)
```
:::


::: {.fragment .callout-tip icon="false"} 
### Predict a text 

```{python}
#| echo: true

X = tm.transform(['Buenos días']) # good morning
m.predict(X)
```
:::


## Performance 

::: {.callout-tip icon="false"} 
### Test set 

```{python} 
#| echo: true

test_set = list(tweet_iterator(fname.replace('_train.', '_test.')))
``` 
::: 

::: {.callout-tip icon="false"} 
### Prediction 

```{python}
#| echo: true

tm = TextModel(token_list=[-2, -1, 3, 4], num_option=OPTION_NONE,
               usr_option=OPTION_DELETE, url_option=OPTION_DELETE,
               emo_option=OPTION_NONE, lc=True, del_dup=False,
               del_punc=True, del_diac=True
              ).fit(training_set)
X = tm.transform(training_set)
labels = np.array([x['klass'] for x in training_set])
m = LinearSVC(dual='auto', class_weight='balanced').fit(X, labels)
hy = m.predict(tm.transform(test_set))
``` 
:::

::: {.callout-tip icon="false"} 
### Performance 

```{python} 
#| echo: true

recall_score([x['klass'] for x in test_set],
             hy, average=None)
```
:::

## Feature Importance 

::: {.callout-tip icon="false"} 
### Coefficients 

```{python} 
#| echo: true

def coef(X, y):
    m = LinearSVC(dual='auto',
                  class_weight='balanced'
                 ).fit(X, y)
    return m.coef_
```
:::

::: {.callout-tip icon="false"} 
### Normalize Coefficients 

```{python} 
#| echo: true

stats = StatisticSamples(statistic=coef,
                         num_samples=50,
                         n_jobs=-1)
b_samples = stats(X, labels)
se = np.std(b_samples, axis=0)
se[se==0] = 1
w_norm = m.coef_ / se
w_norm = np.linalg.norm(w_norm, axis=0)
```
:::

## Feature Importance (2)  

::: {.callout-tip icon="false"} 
### Wordcloud 
```{python} 
#| echo: true
#| output-location: slide
#| fig-pos: center

path = './emoji_text.ttf'
items = tm.token_weight.items
tokens = {tm.id2token[id]: w_norm[id] * _w for id, _w in items()
          if w_norm[id] >= 2.0 and np.isfinite(w_norm[id])}
word_cloud = WordCloud(font_path=path,
                       background_color='white'
                      ).generate_from_frequencies(tokens)
plt.imshow(word_cloud, interpolation='bilinear')
plt.tick_params(left=False, right=False, labelleft=False,
                   labelbottom=False, bottom=False)
```
:::


# Conclusions 

* Describe a supervised learning approach to tackle text classifications.
* Explain the geometry of linear classifiers.
* Use a procedure to represent a text as a vector.
* Measure the performance of a text classifier.