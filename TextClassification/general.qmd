---
lang: en
title: "Text Classification"
# subtitle: "Poornima Institute of Engineering & Technology"
author:
    - name: "Mario Graff"
      url: https://mgraffg.github.io
      affiliation: INFOTEC
    - name: "Daniela Moctezuma"
      url: https://dmocteo.github.io/
      affiliation: "CentroGEO"
    - name: "Eric S. Téllez"
      url: https://sadit.github.io
      affiliation: INFOTEC 
    - name: "Sabino Miranda-Jiménez"

format:
  revealjs:
    theme: simple
    chalkboard: true
    footer: <https://ingeotec.github.io>
---


# Who / Where 

## INGEOTEC

![](../IberLEF2023/ingeotec-cuatro.png){width=200}

::: {style="font-size: 70%; text-align: center;"}  
GitHub: <https://github.com/INGEOTEC>  
WebPage: <https://ingeotec.github.io/>
:::


## Aguascalientes, México

![](aguascalientes_edo.jpg){width=200}

# Introduction 

## Text Classification

::: {.callout-note icon=false}
## Definition
The aim is the **classification** of **documents** into a fixed number of predefined categories. 
::: 

::: {.callout-tip icon=false}
## Polarity
El día de mañana no podré ir con ustedes a la librería
:::

::: {.fragment style="font-size: 100%; text-align: center;"}
**Negative**
:::


## Text Classification Tasks

::: {.callout-tip icon=false}
## Polarity
Positive, Negative, Neutral
:::

::: {.callout-tip icon=false}
## Emotion (Multiclass)
* Anger, Joy, ...
* Intensity of an emotion
:::

::: {.callout-tip icon=false}
## Event (Binary)
* Violent
* Crime
:::


## Profiling

::: {.callout-tip icon=false}
## Gender
Man, Woman, Nonbinary, ...   
:::

::: {.callout-tip icon=false}
## Age
Child, Teen, Adult
:::

::: {.callout-tip icon=false}
## Language Variety
* Spanish: Spain, Cuba, Argentine, México, ...
* English: United States, England, ...
:::


# Approach 

## Machine Learning 

::: {.callout-note icon=false}
### Definition
Machine learning (ML) is a subfield of artificial intelligence that focuses on the development and implementation of algorithms capable of learning from data without being explicitly programmed.
:::

::: {.callout-note icon=false}
### Types of ML algorithms
* Unsupervised Learning
* Supervised Learning
* Reinforcement Learning
:::

## Supervised Learning (Multiclass) 
```{python}
#| echo: false
import plotly.express as px
import plotly.graph_objects as go
from sklearn import decomposition
from sklearn.datasets import load_iris
from sklearn.svm import LinearSVC
import pandas as pd
import numpy as np

D, y = load_iris(return_X_y=True)
pca = decomposition.PCA(n_components=2).fit(D)
Dn = pca.transform(D)
df = pd.DataFrame(Dn, columns=['x', 'y'])
df['Class'] = y.astype(str)
fig = px.scatter(df, color='Class', x='x', y='y', height=400)
fig.update_layout(yaxis={'visible': True, 'showticklabels': False}, 
                  xaxis={'visible': True, 'showticklabels': False},
                  xaxis_title=None, yaxis_title=None)
fig.show()
```

## Supervised Learning (Binary) 

```{python}
#| echo: false

df['Class'] = ['P' if i == 0 else 'N' for i in y]
fig = px.scatter(df, color='Class', x='x', y='y', height=400)
fig.update_layout(yaxis={'visible': True, 'showticklabels': False}, 
                  xaxis={'visible': True, 'showticklabels': False},
                  xaxis_title=None, yaxis_title=None)
fig.show()
```

## Supervised Learning (Classification) 

```{python}
#| echo: false

df['Class'] = ['P' if i == 0 else 'N' for i in y]
m = LinearSVC(dual='auto').fit(Dn, [1 if i == 0 else 0 for i in y])
w_1, w_2 = m.coef_[0]
w_0 = m.intercept_[0]
g_0 = [dict(x1=x, x2=y, tipo='g(x)=0')
       for x, y in zip(Dn[:, 0], (-w_0 - w_1 * Dn[:, 0]) / w_2)]
line = go.Scatter(x=Dn[:, 0], y=(-w_0 - w_1 * Dn[:, 0]) / w_2,
                  line=dict(color='darkgrey'),
                  showlegend=False)
fig = px.scatter(df, color='Class', x='x', y='y', height=400)

fig.update_layout(yaxis={'visible': True, 'showticklabels': False}, 
                  xaxis={'visible': True, 'showticklabels': False},
                  xaxis_title=None, yaxis_title=None)
fig.add_trace(line)
fig.show()
```

::: {.fragment .callout-tip icon="false"}
### Decision function
```{python}
#| echo: false
from IPython.display import Markdown
Markdown(f'$g(\mathbf x) = {w_1:0.2f} x_1 + {w_2:0.2f} x_2 + {w_0:0.2f}$')
```
:::

## Supervised Learning (Geometry)

```{python}
#| echo: false

df['Class'] = ['P' if i == 0 else 'N' for i in y]
m = LinearSVC(dual='auto').fit(Dn, [1 if i == 0 else 0 for i in y])
w_1, w_2 = m.coef_[0]
w_0 = m.intercept_[0]
g_0 = [dict(x1=x, x2=y, tipo='g(x)=0')
       for x, y in zip(Dn[:, 0], (-w_0 - w_1 * Dn[:, 0]) / w_2)]
line = go.Scatter(x=Dn[:, 0], y=(-w_0 - w_1 * Dn[:, 0]) / w_2,
                  line=dict(color='darkgrey'),
                  showlegend=False)
fig = px.scatter(df, color='Class', x='x', y='y', height=400)
# fig.update_layout(yaxis={'visible': True, 'showticklabels': False}, 
#                   xaxis={'visible': True, 'showticklabels': False},
                  # xaxis_title=None, yaxis_title=None)
fig.add_trace(line)
point = go.Scatter(x=[w_1], y=[w_2],
                  line=dict(color='darkgrey'),
                  showlegend=False)
fig.add_trace(point)
fig.show()
```

::: {.callout-tip icon="false"}
### Decision function
```{python}
#| echo: false
from IPython.display import Markdown
Markdown(f'$g(\mathbf x) = {w_1:0.2f} x_1 + {w_2:0.2f} x_2 + {w_0:0.2f}$')
```
:::


## Supervised Learning (Geometry 2)

```{python}
#| echo: false

df['Class'] = ['P' if i == 0 else 'N' for i in y]
m = LinearSVC(dual='auto').fit(Dn, [1 if i == 0 else 0 for i in y])
w_1, w_2 = m.coef_[0]
w_0 = m.intercept_[0]
g_0 = [dict(x1=x, x2=y, tipo='g(x)=0')
       for x, y in zip(Dn[:, 0], (-w_0 - w_1 * Dn[:, 0]) / w_2)]
line = go.Scatter(x=Dn[:, 0], y=(-w_0 - w_1 * Dn[:, 0]) / w_2,
                  line=dict(color='darkgrey'),
                  showlegend=False)
w_00 = 0.88
g_0 = [dict(x1=x, x2=y, tipo='g(x)=0')
       for x, y in zip(Dn[:, 0], (-w_00 - w_1 * Dn[:, 0]) / w_2)]
line2 = go.Scatter(x=Dn[:, 0], y=(-w_00 - w_1 * Dn[:, 0]) / w_2,
                   line=dict(color='black'),
                   showlegend=False)
fig = px.scatter(df, color='Class', x='x', y='y', height=370)
# fig.update_layout(yaxis={'visible': True, 'showticklabels': False}, 
#                   xaxis={'visible': True, 'showticklabels': False},
                  # xaxis_title=None, yaxis_title=None)
fig.add_trace(line)
fig.add_trace(line2)
point = go.Scatter(x=[w_1], y=[w_2],
                  line=dict(color='darkgrey'),
                  showlegend=False)
fig.add_trace(point)
fig.show()
```

::: {.callout-tip icon="false"}
### $w_0$
```{python}
#| echo: false
from IPython.display import Markdown
_ = [f'* $w_0 = {w_0:0.2f}$',
     f'* $w_0 = {w_00:0.2f}$']
Markdown('\n'.join(_))
```
:::


## Supervised Learning (Geometry 3)

```{python}
#| echo: false
#| fig-align: center
from sklearn.linear_model import LogisticRegression
df['Class'] = ['P' if i == 0 else 'N' for i in y]
m = LinearSVC(dual='auto').fit(Dn, [1 if i == 0 else 0 for i in y])
w_1, w_2 = m.coef_[0]
w_0 = m.intercept_[0]
g_0 = [dict(x1=x, x2=y, tipo='g(x)=0')
       for x, y in zip(Dn[:, 0], (-w_0 - w_1 * Dn[:, 0]) / w_2)]
line = go.Scatter(x=Dn[:, 0], y=(-w_0 - w_1 * Dn[:, 0]) / w_2,
                  line=dict(color='darkgrey'),
                  showlegend=False)
eq1 = f'* $g_{{svm}}(\mathbf x) = {w_1:0.2f} x_1 + {w_2:0.2f} x_2 + {w_0:0.2f}$'
point1 = go.Scatter(x=[w_1], y=[w_2],
                  line=dict(color='darkgrey'),
                  showlegend=False)

m = LogisticRegression().fit(Dn, [1 if i == 0 else 0 for i in y])
w_1, w_2 = m.coef_[0]
w_0 = m.intercept_[0]
g_0 = [dict(x1=x, x2=y, tipo='g(x)=0')
       for x, y in zip(Dn[:, 0], (-w_0 - w_1 * Dn[:, 0]) / w_2)]
line2 = go.Scatter(x=Dn[:, 0], y=(-w_0 - w_1 * Dn[:, 0]) / w_2,
                   line=dict(color='black'),
                   showlegend=False)
eq2 = f'* $g_{{lr}}(\mathbf x) = {w_1:0.2f} x_1 + {w_2:0.2f} x_2 + {w_0:0.2f}$'
point2 = go.Scatter(x=[w_1], y=[w_2],
                  line=dict(color='black'),
                  showlegend=False)

fig = px.scatter(df, color='Class', x='x', y='y', height=370)
# fig.update_layout(yaxis={'visible': True, 'showticklabels': False}, 
#                   xaxis={'visible': True, 'showticklabels': False},
                  # xaxis_title=None, yaxis_title=None)
fig.add_trace(line)
fig.add_trace(line2)

fig.add_trace(point1)
fig.add_trace(point2)
fig.show()  
```

::: {.callout-tip icon="false"}
### Decision function
```{python}
#| echo: false
from IPython.display import Markdown
_ = [eq1, eq2]
Markdown('\n'.join(_))
```
:::

# Starting point 

## Training set

```{python}
#| echo: false
from EvoMSA.utils import Download
from microtc.utils import tweet_iterator
from os.path import isdir, isfile
import pandas as pd
from random import shuffle
if not isfile('delitos.zip'):
  Download('https://github.com/INGEOTEC/Delitos/releases/download/Datos/delitos.zip',
           'delitos.zip')
if not isdir('delitos'):
  !unzip -Pingeotec delitos.zip
```

::: {style="font-size: 70%; text-align: center;"}
```{python}
#| echo: false
_ = list(tweet_iterator('delitos/delitos_ingeotec_Es_train.json'))
shuffle(_)
neg = [x for x in _ if x['klass']==0]
pos = [x for x in _ if x['klass']==1]
D = []
for a, b in zip(neg, pos):
  D.append(a)
  D.append(b)
df = pd.DataFrame(D)
df[['text', 'klass']].head(n=8)
```
:::

## Training set (2) 


::: {.callout-note icon=false}
# Problem
The independent variables are **texts**
:::

::: {.fragment .callout-tip icon=false}
# Solution
* Represent the **texts** in an suitable format for the classifier
  * Token as a vector
    * Sparse vector
    * Dense vector
  * Utterance as a vector
:::


# Text Representation 

## Token as Vector 

::: {.callout-note icon="false"}
### Token as vector 

* The idea is that each token $t$ is associate to a vector $\mathbf v_t \in \mathbb R^d$
* Let $\mathcal V$ represent the set composed by the different tokens
* $d$ corresponds to the dimension of the vector
:::

::: {.callout-tip icon="false"}
### $d << \lvert \mathcal V \rvert$ (Dense Vector) 

* GloVe
* Word2vec
* fastText
:::

## Token as Vector (2) 

::: {.callout-note icon="false"}
### $d = \lvert \mathcal V \rvert$ (Sparse Vector)

* $\forall_{i \neq j} \mathbf v_i \cdot \mathbf v_j = 0$
* $\mathbf v_i \in \mathbb R^d$
* $\mathbf v_j \in \mathbb R^d$
:::

::: {.callout-tip icon="false"}
### Algorithm 

:::: columns

::: {.column}
* Sort the vocabulary $\mathcal V$ 
* Associate $i$-th token  to 
$$(\ldots, 0, \overbrace{\beta_i}^i, 0, \ldots)^\intercal$$
:::

::: {.column}
* where $\beta_i > 0$
:::
::: 
::::


## Utterance as Vector 

::: {.callout-note icon="false"}
### Bag of Words 

$$\mathbf x = \sum_{t \in \mathcal U} \mathbf v_t$$

* where $\mathcal U$ corresponds to all the tokens of the utterance
* $\mathbf v_t$ is the vector associate to token $t$
:::

