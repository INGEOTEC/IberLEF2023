---
lang: en
title: "Sentiment Analysis"
subtitle: "Poornima Institute of Engineering & Technology"
author:
    - name: "Mario Graff"
      url: https://mgraffg.github.io
      affiliation: INFOTEC
    # - name: "Daniela Moctezuma"
    #   url: https://dmocteo.github.io/
    #   affiliation: "CentroGEO"
    # - name: "Eric S. Téllez"
    #   url: https://sadit.github.io
    #   affiliation: INFOTEC 
    # - name: "Sabino Miranda-Jiménez"
    #   affiliation: INFOTEC

format:
  revealjs:
    theme: simple
    chalkboard: true
    footer: <https://colab.research.google.com/drive/1P4sE9oEKi7U5lFcKW4XAJfWZvNxvWVDU?usp=sharing>    
---


# Who / Where 

## INGEOTEC

![](../IberLEF2023/ingeotec-cuatro.png){width=200}

::: {style="font-size: 70%; text-align: center;"}  
GitHub: <https://github.com/INGEOTEC>  
WebPage: <https://ingeotec.github.io/>
:::

## Aguascalientes, México

![](aguascalientes_edo.jpg){width=200}

# Introduction 

## Text Classification

::: {.callout-note icon=false}
## Definition
The aim is the **classification** of **documents** into a fixed number of predefined categories. 
::: 

::: {.callout-tip icon=false}
## Polarity
El día de mañana no podré ir con ustedes a la librería
:::

::: {.fragment style="font-size: 100%; text-align: center;"}
**Negative**
:::

## Text Classification Tasks

::: {.callout-tip icon=false}
## Polarity
Positive, Negative, Neutral
:::

::: {.callout-tip icon=false}
## Emotion (Multiclass)
* Anger, Joy, ...
* Intensity of an emotion
:::

::: {.callout-tip icon=false}
## Event (Binary)
* Violent
* Crime
:::

## Profiling

::: {.callout-tip icon=false}
## Gender
Man, Woman, Nonbinary, ...   
:::

::: {.callout-tip icon=false}
## Age
Child, Teen, Adult
:::

::: {.callout-tip icon=false}
## Language Variety
* Spanish: Spain, Cuba, Argentine, México, ...
* English: United States, England, ...
:::

# Approach 

## Machine Learning 

::: {.callout-note icon=false}
### Definition
Machine learning (ML) is a subfield of artificial intelligence that focuses on the development and implementation of algorithms capable of learning from data without being explicitly programmed.
:::

::: {.callout-note icon=false}
### Types of ML algorithms
* Unsupervised Learning
* Supervised Learning
* Reinforcement Learning
:::

## Supervised Learning (Multiclass) 
```{python}
#| echo: false
import plotly.express as px
import plotly.graph_objects as go
from sklearn import decomposition
from sklearn.datasets import load_iris
from sklearn.svm import LinearSVC
import pandas as pd
import numpy as np

D, y = load_iris(return_X_y=True)
pca = decomposition.PCA(n_components=2).fit(D)
Dn = pca.transform(D)
df = pd.DataFrame(Dn, columns=['x', 'y'])
df['Class'] = y.astype(str)
fig = px.scatter(df, color='Class', x='x', y='y', height=400)
fig.update_layout(yaxis={'visible': True, 'showticklabels': False}, 
                  xaxis={'visible': True, 'showticklabels': False},
                  xaxis_title=None, yaxis_title=None)
fig.show()
```

## Supervised Learning (Binary) 

```{python}
#| echo: false

df['Class'] = ['P' if i == 0 else 'N' for i in y]
fig = px.scatter(df, color='Class', x='x', y='y', height=400)
fig.update_layout(yaxis={'visible': True, 'showticklabels': False}, 
                  xaxis={'visible': True, 'showticklabels': False},
                  xaxis_title=None, yaxis_title=None)
fig.show()
```

## Supervised Learning (Classification) 

```{python}
#| echo: false

df['Class'] = ['P' if i == 0 else 'N' for i in y]
m = LinearSVC(dual='auto').fit(Dn, [1 if i == 0 else 0 for i in y])
w_1, w_2 = m.coef_[0]
w_0 = m.intercept_[0]
g_0 = [dict(x1=x, x2=y, tipo='g(x)=0')
       for x, y in zip(Dn[:, 0], (-w_0 - w_1 * Dn[:, 0]) / w_2)]
line = go.Scatter(x=Dn[:, 0], y=(-w_0 - w_1 * Dn[:, 0]) / w_2,
                  line=dict(color='darkgrey'),
                  showlegend=False)
fig = px.scatter(df, color='Class', x='x', y='y', height=400)

fig.update_layout(yaxis={'visible': True, 'showticklabels': False}, 
                  xaxis={'visible': True, 'showticklabels': False},
                  xaxis_title=None, yaxis_title=None)
fig.add_trace(line)
fig.show()
```

::: {.fragment .callout-tip icon="false"}
### Decision function
```{python}
#| echo: false
from IPython.display import Markdown
Markdown(f'$g(\mathbf x) = {w_1:0.2f} x_1 + {w_2:0.2f} x_2 + {w_0:0.2f}$')
```
:::

## Supervised Learning (Geometry)

```{python}
#| echo: false

df['Class'] = ['P' if i == 0 else 'N' for i in y]
m = LinearSVC(dual='auto').fit(Dn, [1 if i == 0 else 0 for i in y])
w_1, w_2 = m.coef_[0]
w_0 = m.intercept_[0]
g_0 = [dict(x1=x, x2=y, tipo='g(x)=0')
       for x, y in zip(Dn[:, 0], (-w_0 - w_1 * Dn[:, 0]) / w_2)]
line = go.Scatter(x=Dn[:, 0], y=(-w_0 - w_1 * Dn[:, 0]) / w_2,
                  line=dict(color='darkgrey'),
                  showlegend=False)
fig = px.scatter(df, color='Class', x='x', y='y', height=400)
# fig.update_layout(yaxis={'visible': True, 'showticklabels': False}, 
#                   xaxis={'visible': True, 'showticklabels': False},
                  # xaxis_title=None, yaxis_title=None)
fig.add_trace(line)
point = go.Scatter(x=[w_1], y=[w_2],
                  line=dict(color='darkgrey'),
                  showlegend=False)
fig.add_trace(point)
fig.show()
```

::: {.callout-tip icon="false"}
### Decision function
```{python}
#| echo: false
from IPython.display import Markdown
Markdown(f'$g(\mathbf x) = {w_1:0.2f} x_1 + {w_2:0.2f} x_2 + {w_0:0.2f}$')
```
:::

## Supervised Learning (Geometry 2)

```{python}
#| echo: false

df['Class'] = ['P' if i == 0 else 'N' for i in y]
m = LinearSVC(dual='auto').fit(Dn, [1 if i == 0 else 0 for i in y])
w_1, w_2 = m.coef_[0]
w_0 = m.intercept_[0]
g_0 = [dict(x1=x, x2=y, tipo='g(x)=0')
       for x, y in zip(Dn[:, 0], (-w_0 - w_1 * Dn[:, 0]) / w_2)]
line = go.Scatter(x=Dn[:, 0], y=(-w_0 - w_1 * Dn[:, 0]) / w_2,
                  line=dict(color='darkgrey'),
                  showlegend=False)
w_00 = 0.88
g_0 = [dict(x1=x, x2=y, tipo='g(x)=0')
       for x, y in zip(Dn[:, 0], (-w_00 - w_1 * Dn[:, 0]) / w_2)]
line2 = go.Scatter(x=Dn[:, 0], y=(-w_00 - w_1 * Dn[:, 0]) / w_2,
                   line=dict(color='black'),
                   showlegend=False)
fig = px.scatter(df, color='Class', x='x', y='y', height=370)
# fig.update_layout(yaxis={'visible': True, 'showticklabels': False}, 
#                   xaxis={'visible': True, 'showticklabels': False},
                  # xaxis_title=None, yaxis_title=None)
fig.add_trace(line)
fig.add_trace(line2)
point = go.Scatter(x=[w_1], y=[w_2],
                  line=dict(color='darkgrey'),
                  showlegend=False)
fig.add_trace(point)
fig.show()
```

::: {.callout-tip icon="false"}
### $w_0$
```{python}
#| echo: false
from IPython.display import Markdown
_ = [f'* $w_0 = {w_0:0.2f}$',
     f'* $w_0 = {w_00:0.2f}$']
Markdown('\n'.join(_))
```
:::

## Supervised Learning (Geometry 3)

```{python}
#| echo: false
#| fig-align: center
from sklearn.linear_model import LogisticRegression
df['Class'] = ['P' if i == 0 else 'N' for i in y]
m = LinearSVC(dual='auto').fit(Dn, [1 if i == 0 else 0 for i in y])
w_1, w_2 = m.coef_[0]
w_0 = m.intercept_[0]
g_0 = [dict(x1=x, x2=y, tipo='g(x)=0')
       for x, y in zip(Dn[:, 0], (-w_0 - w_1 * Dn[:, 0]) / w_2)]
line = go.Scatter(x=Dn[:, 0], y=(-w_0 - w_1 * Dn[:, 0]) / w_2,
                  line=dict(color='darkgrey'),
                  showlegend=False)
eq1 = f'* $g_{{svm}}(\mathbf x) = {w_1:0.2f} x_1 + {w_2:0.2f} x_2 + {w_0:0.2f}$'
point1 = go.Scatter(x=[w_1], y=[w_2],
                  line=dict(color='darkgrey'),
                  showlegend=False)

m = LogisticRegression().fit(Dn, [1 if i == 0 else 0 for i in y])
w_1, w_2 = m.coef_[0]
w_0 = m.intercept_[0]
g_0 = [dict(x1=x, x2=y, tipo='g(x)=0')
       for x, y in zip(Dn[:, 0], (-w_0 - w_1 * Dn[:, 0]) / w_2)]
line2 = go.Scatter(x=Dn[:, 0], y=(-w_0 - w_1 * Dn[:, 0]) / w_2,
                   line=dict(color='black'),
                   showlegend=False)
eq2 = f'* $g_{{lr}}(\mathbf x) = {w_1:0.2f} x_1 + {w_2:0.2f} x_2 + {w_0:0.2f}$'
point2 = go.Scatter(x=[w_1], y=[w_2],
                  line=dict(color='black'),
                  showlegend=False)

fig = px.scatter(df, color='Class', x='x', y='y', height=370)
# fig.update_layout(yaxis={'visible': True, 'showticklabels': False}, 
#                   xaxis={'visible': True, 'showticklabels': False},
                  # xaxis_title=None, yaxis_title=None)
fig.add_trace(line)
fig.add_trace(line2)

fig.add_trace(point1)
fig.add_trace(point2)
fig.show()  
```

::: {.callout-tip icon="false"}
### Decision function
```{python}
#| echo: false
from IPython.display import Markdown
_ = [eq1, eq2]
Markdown('\n'.join(_))
```
:::

# Starting point 

## Training set

```{python}
#| include: false
from EvoMSA.utils import Download
from microtc.utils import tweet_iterator
from os.path import isdir, isfile
import pandas as pd
from random import shuffle
if not isfile('hi-regions.json.zip'):
  Download('https://github.com/INGEOTEC/talks/releases/download/Talks/hi-regions.json.zip', 'hi-regions.json.zip')
if not isfile('hi-regions.json'):
  !unzip -Pingeotec hi-regions.json.zip
```

::: {style="font-size: 70%; text-align: center;"}
```{python}
#| echo: false
D = list(tweet_iterator('hi-regions.json'))
shuffle(D)
df = pd.DataFrame(D)
df[['text', 'klass']].head(n=8)
```
:::

## Quiz 

::: {.callout-warning icon="false"}
### Question 

Which of the following tasks does the previous training set belong to?

1. Polarity
2. Emotion identification
3. Aggressive detection
4. Profiling
:::

## Training set (2) 

::: {.callout-note icon=false}
### Problem
The independent variables are **texts**
:::

::: {.fragment .callout-tip icon=false}
### Solution
* Represent the **texts** in an suitable format for the classifier
  * Token as a vector
    * Sparse vector
    * Dense vector
  * Utterance as a vector
:::

# Text Representation 

## Token as Vector 

::: {.callout-note icon="false"}
### Token as vector 

* The idea is that each token $t$ is associate to a vector $\mathbf v_t \in \mathbb R^d$
* Let $\mathcal V$ represent the set composed by the different tokens
* $d$ corresponds to the dimension of the vector
:::

::: {.fragment .callout-tip icon="false"}
### $d << \lvert \mathcal V \rvert$ (Dense Vector) 

* GloVe
* Word2vec
* fastText
:::

## Token as Vector (2) 

::: {.callout-note icon="false"}
### $d = \lvert \mathcal V \rvert$ (Sparse Vector)

* $\forall_{i \neq j} \mathbf v_i \cdot \mathbf v_j = 0$
* $\mathbf v_i \in \mathbb R^d$
* $\mathbf v_j \in \mathbb R^d$
:::

::: {.fragment .callout-tip icon="false"}
### Algorithm 

* Sort the vocabulary $\mathcal V$ 
* Associate $i$-th token  to 
* $(\ldots, 0, \overbrace{\beta_i}^i, 0, \ldots)^\intercal$
* where $\beta_i > 0$
:::

## Utterance as Vector 

::: {.callout-note icon="false"}
### Procedure 

$$\mathbf x = \sum_{t \in \mathcal U} \mathbf{v}_t$$

* where $\mathcal{U}$ corresponds to all the tokens of the utterance
* The vector $\mathbf{v}_t$ is associated to token $t$
:::

::: {.callout-tip icon="false" .fragment} 
### Unit Vector 

$$\mathbf x = \frac{\sum_{t \in \mathcal U} \mathbf v_t}{\lVert \sum_{t \in \mathcal U} \mathbf v_t \rVert} $$ 

::: 

## Tokens 

```{mermaid}
%%| echo: false
%%| fig-align: center

flowchart LR
    Entrada([Text]) -->  Norm[Text Normalizer]
    Norm --> Seg[Tokenizer]
    Seg --> Terminos(...)
```

:::: {.columns}
::: {.fragment .callout-tip icon="false" .column width=40%}
### Text Normalization

* User
* URL
* Entity
* Case sensitive
* Punctuation
* Diacritic
:::
::: {.fragment .callout-tip icon="false" .column}
### Diacritic (remove) 

```{python}
#| echo: true
import unicodedata
text = 'México'
output = ""
for x in unicodedata.normalize('NFD', text):
    o = ord(x)
    if 0x300 <= o and o <= 0x036F:
        continue
    output += x
output
```
:::
::::

## Text Normalization 

::: {.callout-tip icon="false"} 
### Case sensitive 

```{python}
#| echo: true
text = "México"
output = text.lower()
output
```
:::

::: {.callout-tip icon="false"} 
### User (replace) 

```{python}
#| echo: true
import re
text = "go http://google.com, and find out"
output = re.sub(r"https?://\S+", "_usr", text)
output
```
:::

## Tokenizer 

::: {.callout-note icon="false"}
### Common Types 

* Words
* n-grams (Words)
* q-grams (Characters)
* skip-grams
:::

::: {.fragment .callout-tip icon="false"}
### Words 

```{python}
#| echo: true
text = 'I like playing football on Saturday'
words = text.split()
words
```
:::

## Tokenizer (2)

:::: {.columns}
::: {.callout-tip icon="false" .column}
### n-grams 

```{python}
#| echo: true
text = 'I like playing football on Saturday'
words = text.split()
n = 3
n_grams = []
for a in zip(*[words[i:] for i in range(n)]):
    n_grams.append("~".join(a))
n_grams
```
:::
::: {.fragment .callout-tip icon="false" .column}
### q-grams 

```{python}
#| echo: true
text = 'I like playing'
q = 4
q_grams = []
for a in zip(*[text[i:] for i in range(q)]):
    q_grams.append("".join(a))
q_grams
```
:::
::::

## $\mu$-TC 

::: {.callout-tip icon="false"}
### TextModel 

```{python}
#| echo: true
from microtc import TextModel
from microtc.params import OPTION_GROUP,\
  OPTION_DELETE, OPTION_NONE
tm = TextModel(token_list=[-1],
               num_option=OPTION_NONE,
               usr_option=OPTION_DELETE,
               url_option=OPTION_DELETE,
               emo_option=OPTION_NONE,
               lc=True,
               del_dup=False,
               del_punc=True,
               del_diac=True)
```
:::
::: {.fragment}
```{python}
#| echo: true
text = 'I like playing football with @mgraffg'
tm.tokenize(text)
```
:::

## $\mu$-TC (2) 

:::: {.columns}
::: {.callout-tip icon="false" .column}
### TextModel 

```{python}
#| echo: true
tm = TextModel(token_list=[-2, -1, 6],
               num_option=OPTION_NONE,
               usr_option=OPTION_DELETE,
               url_option=OPTION_DELETE,
               emo_option=OPTION_NONE, 
               lc=True, del_dup=False,
               del_punc=True, del_diac=True)
```
:::
::: {.fragment .column}
```{python}
#| echo: true
text = 'I like playing...'
tm.tokenize(text)
```
:::
::::

## Training set

```{python}
#| echo: true
from EvoMSA.utils import Download
from microtc.utils import tweet_iterator
from os.path import isdir, isfile
import pandas as pd
from random import shuffle
URL = 'https://github.com/INGEOTEC/talks/releases/download/Talks/hi-regions.json.zip'
if not isfile('hi-regions.json.zip'):
  Download(URL, 'hi-regions.json.zip')
if not isfile('hi-regions.json'):
  !unzip -Pingeotec hi-regions.json.zip
```

## Utterance as Vector 

:::: {.columns}
::: {.callout-tip icon="false" .column}
### TextModel 

```{python}
#| echo: true
tm = TextModel(token_list=[-1],
               num_option=OPTION_NONE,
               usr_option=OPTION_DELETE,
               url_option=OPTION_DELETE,
               emo_option=OPTION_NONE, 
               lc=True, del_dup=False,
               del_punc=True, del_diac=True)
```               
:::

::: {.fragment .callout-tip icon="false" .column}
### Tokenizer  

```{python}
#| echo: true
from microtc.utils import tweet_iterator
fname = 'hi-regions.json'
training_set = list(tweet_iterator(fname))
tm.tokenize(training_set[0])[:3]
```
:::
::::

::: {.fragment .callout-tip icon="false"}
### Vocabulary 

```{python} 
#| echo: true
from microtc.utils import Counter
voc = Counter()
for text in training_set:
  tokens = set(tm.tokenize(text))
  voc.update(tokens)
voc.most_common(n=3)
```
:::


## Utterance as Vector (2)

::: {.callout-tip icon="false"}
### Inverse Document Frequency (IDF)

```{python}
#| echo: true
token2id = {}
token2beta = {}
N = np.log2(voc.update_calls)
for id, (k, n) in enumerate(voc.items()):
  token2id[k] = id
  token2beta[k] = N - np.log2(n)
```               
:::

::: {.fragment .callout-tip icon="false"}
### Term Frequency - IDF
```{python} 
#| echo: true
text = training_set[3]['text']
tokens = tm.tokenize(text)
vector = []
for token, tf in zip(*np.unique(tokens, return_counts=True)):
  if token not in token2id:
    continue
  vector.append((token2id[token], tf * token2beta[token]))
vector[:4]
```
:::

## Utterance as Vector (3)

::: {.callout-tip icon="false"}
### $\mu$-TC

```{python}
#| echo: true
tm.fit(training_set)
```               
:::

::: {.callout-tip icon="false"}
### Utterance as Vector 
```{python} 
#| echo: true
text = training_set[3]['text']
tm[text][:4]
```
:::

## Quiz 

::: {.callout-warning icon="false"}
### Question 

Which of the following representations do you consider to produce a larger vocabulary?
:::

:::: {.columns}
::: {.callout-tip icon="false" .column}
### A 
```{python}
#| echo: true
tmA = TextModel(token_list=[-1, 3],
                num_option=OPTION_NONE,
                usr_option=OPTION_DELETE,
                url_option=OPTION_DELETE,
                emo_option=OPTION_NONE, 
                lc=True, del_dup=False,
                del_punc=True,
                del_diac=True
               ).fit(training_set)
```
:::

::: {.callout-tip icon="false" .column}
### B 
```{python}
#| echo: true
tmB = TextModel(token_list=[-1, 6],
                num_option=OPTION_NONE,
                usr_option=OPTION_DELETE,
                url_option=OPTION_DELETE,
                emo_option=OPTION_NONE, 
                lc=True, del_dup=False,
                del_punc=True,
                del_diac=True
               ).fit(training_set)
```
:::
::::

# Text Classification 

## Procedure 

::: {.callout-tip icon="false"}
### Text as Vectors 

```{python}
#| echo: true
X = tm.transform(training_set)
```
:::

::: {.fragment .callout-tip icon="false"}
### Training a Classifier 

```{python}
#| echo: true
from sklearn.svm import LinearSVC
labels = [x['klass'] for x in training_set]
m = LinearSVC(dual='auto').fit(X, labels)
```
:::


::: {.fragment .callout-tip icon="false"}
### Predict a text 

```{python}
#| echo: true
X = tm.transform(['Buenos días']) # Good morning
m.predict(X)
```
:::


## Performance 

```{python}
#| echo: true
from sklearn.model_selection import KFold
from sklearn.metrics import recall_score

kfold = KFold(n_splits=5, shuffle=True, random_state=1)
perf = []
for tr, vs in kfold.split(training_set):
    train = [training_set[i] for i in tr]
    val = [training_set[i] for i in vs]
    tm = TextModel(token_list=[-1], num_option=OPTION_NONE,
                   usr_option=OPTION_DELETE, url_option=OPTION_DELETE,
                   emo_option=OPTION_NONE, lc=True, del_dup=False,
                   del_punc=True, del_diac=True).fit(train)
    labels = [x['klass'] for x in train]
    m = LinearSVC(dual='auto').fit(tm.transform(train), labels)
    hy = m.predict(tm.transform(val))
    _ = recall_score([x['klass'] for x in val], hy, average='macro')
    perf.append(_)
np.mean(perf)
```

# Conclusions 

* Describe a supervised learning approach to tackle text classifications.
* Explain the geometry of linear classifiers.
* Use a procedure to represent a text as a vector.
* Measure the performance of a text classifier.
  
::: {.callout-tip icon="false" style="text-align: center;"}
### Personal webpage

<https://mgraffg.github.io>
:::